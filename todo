# TODO
- compress images to reduce storage usage if user picks --compress
    - show compression gains
+ rip from csv list with subreddits
    - test if ripping from csv works..
+ new and improved pushshift iterator
+ call lil ripper using args in terminal
- pytests, because i have like 10% test coverage rn, if not less..
+ use random browser headers.

# Optimization of dupe checking.
- extract download links from all jsons in one list (look at memory usage, trade-off between ripping efficiency and memory complexity?)
    + remove duplicates
    - check if already downloaded
    - start multiple processes and work on list of remaining, yet not downloaded urls

- handle gfycat blacklisting somehow.. play around with random headers? -> does not work.
    - look into proxies? https://www.scrapehero.com/how-to-rotate-proxies-and-ip-addresses-using-python-3/
      but cloudflare probably figured this out before me lol
    - add --gfycat flag that removes multiprocessing and slowly crawls? should reduce requests by a lot with trade-off of long archiving.

- stop execution when pressing ctrl+c and still show stats + cancel all processes!