"""
Land of code that might be helpful or remind me of what the fuck I coded. Scared to delete.
"""


@DeprecationWarning
def extract_data_json_url(json_url):
    # todo perkelt sita shit tiesiai i db thing, kad iskarto is json keltu
    """
    Extracts data from pushshift json url. Returns only needed data for the database in form of list of dictionaries.
    :param json_url: Valid pushshift json url.
    :return: List of dictionaries containing data of each post found in url.
    """
    extracted_data = []
    # Extract data and return useful data
    json_data = requests.get(json_url).json()
    # Goes over each post and extracts useful data.
    for post in json_data["data"]:
        post_data = {"post_id": post["id"], "created_utc": post["created_utc"], "author": post["author"], "media_url": post["url"], }
        extracted_data.append(post_data)
    return extracted_data


@DeprecationWarning
def add_post_data_databaseOLD(subreddit_name, json_data):
    global total_db_updates
    # Given data, import it to db
    # Table name is subreddit name
    # post_id | created_utc | url | title | author | status
    # status is then generated by python -> downloaded, failed
    # maybe add hash to find them better or name so that bot could take them easier
    # todo handle db errors!
    # todo take database name from args or settings.json
    db_connection = sqlite3.connect("../test.sqlite3")
    db_cursor = db_connection.cursor()
    # todo this is not safe for injection attacks, but do i care for this type of program?
    db_cursor.execute('''CREATE TABLE IF NOT EXISTS {} (post_id TEXT PRIMARY KEY, created_utc INTEGER, url TEXT, author TEXT, status TEXT)'''.format(subreddit_name))
    # Add each post as separate entry to db
    for post in json_data:
        db_cursor.execute(
            f'''INSERT OR REPLACE INTO {subreddit_name} VALUES ("{post.get("post_id")}", {post.get("created_utc")}, "{post.get("media_url")}", "{post.get("author")}", NULL)''')
        total_db_updates += 1
    db_connection.commit()
    db_connection.close()


@DeprecationWarning
def pushshift_get_all_subreddit_json_urls(subreddit_name):
    # todo find a way to not index removed, deleted posts? would speed up dramatically
    # todo add params from, to for a date range parsing
    # todo is there a way to get last post created_utc without the whole r.json() thing? ask on le stakoverflow
    # todo handle exception if url could not be opened, https://www.datasciencelearner.com/how-to-get-json-data-from-url-in-python/
    iter = 0  # todo DEBUG PRINT - delete in production release
    """
    Given subreddit name, extracts all json urls containing post data.
    :param subreddit_name: Name of subreddit as string value
    :return: List of lists containing json urls.
    """
    # Store all generated json urls
    json_links = []
    # Get current date in unix timestamp, will be replaced by last unix timestamp from json file.
    current_unix_timestamp = int(time.time())
    # Iterate downwards until we reach the end and error will be thrown, since no posts will be found in json file.
    try:
        while True:
            iter += 1 # todo DEBUG PRINT - delete in production release
            # todo &is_self=false ar jis nepameta kazkiek media posts? need more testing
            json_link = "https://api.pushshift.io/reddit/search/submission/?subreddit={}&sort=desc&sort_type=created_utc&before={}&size=1000&is_self=false".format(
                subreddit_name, current_unix_timestamp)
            r = requests.get(json_link)
            json_link_data = r.json()
            json_links.append(json_link)
            current_unix_timestamp = json_link_data["data"][-1]["created_utc"]


            print(json_link, iter, datetime.utcfromtimestamp(current_unix_timestamp))  # todo DEBUG PRINT - delete in production release
    except IndexError:
        # When all json files are generated, return them to caller
        return json_links


@DeprecationWarning
def parse_subreddit(subreddit_name, time_from=int(time.time()), time_to=1):
    # todo add time frame FROM TO as optional
    # todo default time_from is current unix timestamp and time_to is lowest unix timestamp
    # 1. Get all urls
    # 2. Iterate over all urls and extract data
    # 3. Add that data to database
    # 4. Return
    posts_added_db = 0
    start_time = time.time()
    json_urls = pushshift_get_all_subreddit_json_urls(subreddit_name)
    for json_url in json_urls:
        post_data = extract_data_json_url(json_url)
        posts_added_db += add_post_data_database(subreddit_name, post_data)
    elapsed_time = time.time() - start_time
    print(f"parsed {len(json_urls)} json url's and added {posts_added_db} posts to the database in {elapsed_time} seconds.")


@DeprecationWarning
def pushshift_get_parse_add(subreddit_name):
    # todo time_from -> current time or lowest time in db, time_to is 0 or highest in db
    # todo find a way to not index removed, deleted posts? would speed up dramatically
    # todo add params from, to for a date range parsing
    # todo is there a way to get last post created_utc without the whole r.json() thing? ask on le stakoverflow
    # todo handle exception if url could not be opened, https://www.datasciencelearner.com/how-to-get-json-data-from-url-in-python/
    iter = 0  # todo DEBUG PRINT - delete in production release
    """
    Given subreddit name, extracts all json urls containing post data.
    :param subreddit_name: Name of subreddit as string value
    :return: List of lists containing json urls.
    """
    # Store all generated json urls

    # Get current date in unix timestamp, will be replaced by last unix timestamp from json file.
    current_unix_timestamp = int(time.time())
    # Iterate downwards until we reach the end and error will be thrown, since no posts will be found in json file.
    try:
        while True:
            iter += 1 # todo DEBUG PRINT - delete in production release
            # todo &is_self=false ar jis nepameta kazkiek media posts? need more testing
            json_link = "https://api.pushshift.io/reddit/search/submission/?subreddit={}&sort=desc&sort_type=created_utc&before={}&size=1000&is_self=false".format(
                subreddit_name, current_unix_timestamp)
            r = requests.get(json_link)
            json_link_data = r.json()
            reduced_json = extract_data_json_url(json_link)
            added_items = add_post_data_database(subreddit_name, reduced_json)
            #current_unix_timestamp = json_link_data["data"][-1]["created_utc"]
            current_unix_timestamp = reduced_json[-1].get("created_utc")
            print(json_link, iter, datetime.utcfromtimestamp(current_unix_timestamp))  # todo DEBUG PRINT - delete in production release
    except IndexError:
        # When all json files are generated, return them to caller
        print(f"all {iter} json url's were generated. finished.")


